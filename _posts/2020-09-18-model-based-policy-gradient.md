---
layout: post
title: "Model-based Policy Gradients"
mathjax: true
categories:
  - Home
tags:
  - machine learning
  - reinforcement learning
  - model-based RL
---
![pirandello](/assets/images/pgt.png){:class="img-responsive"}
You recognized it. This is the super-handy expression for the policy gradient derived [more than 20 years ago](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation) by Richard Sutton and colleagues.
In this blog post, I will show how the **Policy Gradient Theorem** can offer a lens to interpret modern model-based policy search methods. Yes, *even the ones that do not directly consider it*.

---

## A recap on the Theorem -- The Model-Free Gradient
Let's first state the theorem using a convenient notation[^1].
Given \\( J \\), the performance of a stochastic differentiable policy \\( \pi_\theta \\), the gradient with respect to its parameters is given by:
<center>
$$ \nabla_\theta J(\theta) = \int d^{\pi}(s,a) \nabla_\theta \log \pi_\theta(a|s) Q^{\pi}(s,a) \mathrm{d}s \mathrm{d}a.$$
</center>
In other words, if you look for an improvement direction for the performance of a policy, you should take three elements into account:
- \\( d^{\pi}(s,a) \\), the  stationary distribution of states and actions in the environment if \\( \pi \\) is executed;
- \\( \nabla_\theta \log \pi_\theta (a\|s) \\), the _score_, linked to the possibility of a change in the parameters of the policy on a given state and action;
- \\( Q^{\pi}(s,a) \\), the expected cumulative discounted return that follows the execution of action \\( a \\) in state \\( s \\).

Since the policy is usually available and the chosen policy class is convenient, computing the score is usually not a problem.
More delicate is instead the matter of how to handle the other two factors, since they depend on the dynamics of the environment \\( p(\cdot \| s, a ) \\).
Indeed, this design choice determines the **type of gradient** that you are going to use to improve the policy, with significant consequences.

To see this more clearly, let's write once again the statement of the Policy Gradient Theorem, making the dependence of the different elements on the environment model \\( p \\) explicit:
<center>
$$ \nabla_\theta J^{MFG}(\theta) = \int d^{\pi,p}(s,a) \nabla_\theta \log \pi_\theta(a|s) Q^{\pi,p}(s,a) \mathrm{d}s \mathrm{d}a,$$
</center>

where the acronym MFG stands for **Model-Free Gradient**, to highlight the fact that only the dynamics from the environment is considered in it.
To sample from \\( d^{\pi,p} \\) and estimate \\( Q^{\pi,p} \\), one can use samples from \\( p(\cdot\|s,a) \\), i.e., real interactions with the real environment: this is at the core of any, old or new, model-free policy gradient algorithm based on stochastic policies.
For instance, by sampling states and actions along trajectories generated by \\( \pi \\) and using the empirical return of the trajectory as estimate
However, interacting with the environment can be costly, and estimating the gradient from samples in this fashion can easily yield _high variance_.

---
## A common alternative: the Fully-Model-based Gradient
One of the biggest promises of model-based reinforcement learning is to increase sample efficiency: to reduce the _variance_ at the cost of introducing a _bias_.
The source for the bias is the use of an estimated model \\(\widehat{p}(\cdot \| s,a) \\), that can substitute the real model when needed.

In policy gradient approaches, this translates into a \\( p \leftrightarrow \widehat{p} \\) substitution into the two quantities that appear in the Policy Gradient Theorem.
The most common way to inject an estimated model into an approximation for the policy gradient leads to the following definition:
<center>
$$ \nabla_\theta J^{FMG}(\theta) = \int d^{\pi,\widehat{p}}(s,a) \nabla_\theta \log \pi_\theta(a|s) Q^{\pi,\widehat{p}}(s,a) \mathrm{d}s \mathrm{d}a,$$
</center>
that we can call **Fully Model-based Gradient**, FMG for short.

---
[^1]: I'll be particularly easy on notation (e.g., omitting some normalization constants).
