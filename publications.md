---
layout: page
title: Publications
sidebar_link: True
sidebar_sort_order: 1
---

You can also find me on [Google Scholar](https://scholar.google.it/citations?user=AuVp7pkAAAAJ&hl=en).

<style>
    .rounded-box {
        background-color: #f0f0f0;
        padding: 16px;
        margin-bottom: 10px;
        border-radius: 8px;
    }

  summary {
    cursor: pointer;
  }


    br {
   display: block;
   margin: 4px 0;
    }

</style>

## Conference and Journal Papers

<div class="rounded-box">
    <strong><a href="https://arxiv.org/abs/2310.00166" target="_blank">Motif: Intrinsic Motivation from Artificial Intelligence Feedback</a></strong><br>
    Martin Klissarov*, Pierluca D’Oro*, Shagun Sodhani, Roberta Raileanu, Pierre-Luc Bacon, Pascal Vincent, Amy Zhang, Mikael Henaff <br>
    2023. <br>
    <em>International Conference on Learning Representations</em>, 2024. <br>
  <details>
    <summary> <b> Behind the scenes </b> </summary>
    <p> <i> At the beginning of summer 2023, a wave of research works applied Large Language Models to sequential decision-making. This caused both excitement and confusion in me, about which I wrote about in a <a href="https://www.scienceofaiagents.com/p/to-keep-doing-rl-research-stop-calling" target="_blank">blog post</a> that was pivotal for me. Seriously, I wanted to know whether there was something really interesting behind the hype. When Martin joined Meta as an intern, during many days of intense brainstorming, we enumerated the possible ways to use LLMs for decision-making. I got pretty convinced that extracting a reward function from them was the most promising of all. In that particularly long, rainy and confusing summer in Montreal, we pushed ourselves out of our comfort zone and witnessed the potential of LLMs for creating AI agents.  </i> </p>
  </details>
</div>

<div class="rounded-box">
    <strong><a href="https://openreview.net/forum?id=M3QXCOTTk4" target="_blank">The Curse of Diversity in Ensemble-Based Exploration</a></strong><br>
    Zhixuan Lin, Pierluca D'Oro, Evgenii Nikishin, Aaron Courville  <br>
    2023. <br>
    <em>International Conference on Learning Representations</em>, 2024. <br>
  <details>
    <summary> <b> Behind the scenes </b> </summary>
    <p> <i> Zhixuan was initially curious to explore the interaction between the resetting mechanisms we have been leveraging in our previous work and ensembling methods for deep reinforcement learning. In the end, in a piece of work on the empirical science of neural networks for reinforcement learning, we discovered surprising phenomena about the interaction among ensembles, data collection and representation learning. </i> </p>
  </details>
</div>


<div class="rounded-box">
  <strong><a href="https://arxiv.org/abs/2309.14597" target="_blank">Policy Optimization in a Noisy Neighborhood: On Return Landscapes in Continuous Control</a></strong><br>
  Nate Rahn*, Pierluca D'Oro*, Harley Wiltzer, Pierre-Luc Bacon, Marc G. Bellemare <br>
  <em>Conference on Neural Information Processing Systems</em>, 2023. <br>
  <details>
    <summary> <b> Behind the scenes </b> </summary>
    <p> <i> Motivated by new discoveries about the empirical science of deep reinforcement learning, we started discussing techniques for discovering other phenomena and advancing our understanding of neural network-based agents. After some attempts and after building an appropriate experimental framework, we came to the conclusion that the lens of the return landscape was a good one for our goal. Funnily enough, we found some new interesting phenomena right when we stopped searching for them. I learned a lot about how to do understanding-oriented science. </i> </p>
  </details>
</div>


<div class="rounded-box">
    <strong><a href="https://openreview.net/forum?id=OpC-9aBBVJe" target="_blank">Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier</a></strong><br>
    Pierluca D'Oro*, Max Schwarzer*, Evgenii Nikishin, Pierre-Luc Bacon, Marc G. Bellemare, Aaron Courville <br>
    <em>International Conference on Learning Representations (oral presentation, notable top 5%)</em>, 2023. <br>
  <details>
    <summary> <b> Behind the scenes </b> </summary>
    <p> <i>  As the paradigm of increasing performance by scaling the amount of computation was being established in the rest of the machine learning community, we were looking for a way to generalize this to reinforcement learning. Guided by some preliminary evidence we had shown in the primacy bias paper, we thought a way to do it was to increase the amount of updates per environment interaction. I had fun doing some research in which the main goal was not to develop a totally new method or to show good performance really, but to go deep with analyses and to try to empirically understand the implications of different aspects of a complex system. </i> </p>
  </details>
</div>

<div class="rounded-box">
    <strong><a href="https://arxiv.org/abs/2205.07802" target="_blank">The Primacy Bias in Deep Reinforcement Learning</a></strong><br>
    Evgenii Nikishin*, Max Schwarzer*, Pierluca D'Oro*, Pierre-Luc Bacon, Aaron Courville <br>
    <em>International Conference on Machine Learning</em>, 2022. <br>
  <details>
    <summary> <b> Behind the scenes </b> </summary>
    <p> <i> Evgenii had shared some interesting results on how resetting parameters in neural networks was sometimes giving unexpected performance gains. In a fun scientific sprint, we tried to understand how general the improvements provided by resets were and where they were coming from. Through this project, I understood the huge power of deeply collaborative research and of intuition-guided empirical science. </i> </p>
  </details>
</div>

<div class="rounded-box">
    <strong><a href="https://arxiv.org/abs/2012.08225" target="_blank">Policy Optimization as Online Learning with Mediator Feedback</a></strong><br>
    Alberto Maria Metelli*, Matteo Papini*, Pierluca D'Oro, Marcello Restelli <br>
    <em>AAAI Conference on Artificial Intelligence</em>, 2021. <br>
  <details>
    <summary> <b> Behind the scenes </b> </summary>
    <p> <i> I moved to Milan in March 2020, one week before the very first Covid lockdown started. I didn't leave my apartment for several weeks, and helped out as an intern with a theory-oriented project. Enduring the lockdown and the pandemic was a life-changing challenge, for me as for almost everybody else.  </i> </p>
  </details>
</div>

<div class="rounded-box">
    <strong><a href="https://arxiv.org/abs/2004.14309" target="_blank">How to Learn a Useful Critic? Model-based Action-Gradient-Estimator Policy Optimization</a></strong><br>
    Pierluca D'Oro, Wojciech Jaśkowski <br>
    <em>Conference on Neural Information Processing Systems</em>, 2020. <br>
  <details>
    <summary> <b> Behind the scenes </b> </summary>
    <p> <i> While living with a daily commute between the wonderful lake city of Como and Switzerland, I tried a bunch of mostly theoretical ideas during my time at NNAISENSE. We found out at some point with my host Wojciech that the ideas I had been thinking about for my master thesis were actually generalizable to actor-critic methods: it implied a simple theory-backed deep reinforcement learning algorithm that yielded good results out-of-the-box. I have established in that occasion, synthesizing my previous experience with what I learned from Wojciech, the core of what would have been my research taste in subsequent years.   </i> </p>
  </details>
</div>

<div class="rounded-box">
    <strong><a href="https://pubs.acs.org/doi/10.1021/acs.analchem.0c00585" target="_blank">SMfinder: Small Molecules Finder for Metabolomics and Lipidomics analysis</a></strong><br>
    Giuseppe Martano, Michele Leone, Pierluca D'Oro, Vittoria Matafora, Angela Cattaneo, Marco Masseroli, Angela Bachi <br>
    <em>Analytical Chemistry</em>, 2020. <br>
  <details>
    <summary> <b> Behind the scenes </b> </summary>
    <p> <i> Michele, a flatmate at that time, told me that he was collaborating with a chemist on creating a platform for the analysis of experimental data. I decided to help, with the goal of learning about cross-disciplinary collaborations. We spent with Michele several evenings building software together in the student residence we were living in. We learned a lot and celebrated small successes with cheap grocery store cake slices.  </i> </p>
  </details>
</div>

<div class="rounded-box">
    <strong><a href="https://arxiv.org/abs/1909.04115" target="_blank">Gradient-Aware Model-based Policy Search</a></strong><br>
    Pierluca D'Oro*, Alberto Maria Metelli*, Andrea Tirinzoni, Matteo Papini, Marcello Restelli <br>
    <em>AAAI Conference on Artificial Intelligence</em>, 2020. <br>
  <details>
    <summary> <b> Behind the scenes </b> </summary>
    <p> <i> At the beginning of my Master's research work, I really had to learn the hard way how to precisely formalize problems and think in math, since I had realized my drawings of boxes on a whiteboard were not enough anymore to express my scientific self. We had in mind the general goal to learn a model of the dynamics that was tailored to its use in reinforcement learning. We ponderer about what that meant exactly, and got inspired by the ideas of Amir-massoud Farahmand on <a href="https://proceedings.mlr.press/v54/farahmand17a.html" target="_blank">decision-aware model learning</a>. I spent several months staring at a whiteboard and thinking about math for most of my time.
 </i> </p>
  </details>
</div>

<div class="rounded-box">
    <strong><a href="https://arxiv.org/abs/1803.09092" target="_blank">Adversarial Framework for Unsupervised Learning of Motion Dynamics in Videos</a></strong><br>
    Concetto Spampinato, Simone Palazzo, Pierluca D’Oro, Daniela Giordano, Mubarak Shah <br>
    <em>International Journal of Computer Vision</em>, 2019. <br>
  <details>
    <summary> <b> Behind the scenes </b> </summary>
    <p> <i> I wanted to have a first experience with scientific research, to learn what it was and to see whether it was fun for me. Concetto told me they were working on video generation and I was very excited to help them. It's probably the first time I've realized you can actually do science as a job for real, when you put enough effort in it. I brainstormed about research ideas, learned how to draw big boxes on a whiteboard, and trained neural networks for the first time. I guess I liked it enough to decide to continue on that path. </i> </p>
  </details>
</div>


## Workshop Papers

<div class="rounded-box">
    <strong><a href="https://openreview.net/forum?id=Rbt3zk4I-yM" target="_blank">Unleashing The Potential of Data Sharing in Ensemble Deep Reinforcement Learning</a></strong><br>
    Zhixuan Lin, Pierluca D'Oro, Evgenii Nikishin, Aaron Courville <br>
    <em>NeurIPS Deep Reinforcement Learning Workshop</em>, 2022.
</div>

<div class="rounded-box">
    <strong><a href="https://sites.google.com/view/deep-rl-workshop-neurips2021" target="_blank">Long-Term Credit Assignment via Model-based Temporal Shortcuts</a></strong><br>
    Michel Ma, Pierluca D'Oro, Yoshua Bengio, Pierre-Luc Bacon <br>
    <em>NeurIPS Deep Reinforcement Learning Workshop</em>, 2021.
</div>

<div class="rounded-box">
    <strong><a href="https://sites.google.com/view/metacogneurips2021/papers" target="_blank">Meta Dynamic Programming</a></strong><br>
    Pierluca D'Oro, Pierre-Luc Bacon <br>
    <em>NeurIPS Workshop on Metacognition in the Age of AI: Challenges and Opportunities</em>, 2021.
</div>

<div class="rounded-box">
    <strong><a href="https://arxiv.org/abs/2004.03156" target="_blank">Real-time Classification from Short Event-Camera Streams using Input-filtering Neural ODEs</a></strong><br>
    Giorgio Giannone, Asha Anoosheh, Alessio Quaglino, Pierluca D'Oro, Marco Gallieri, Jonathan Masci <br>
    <em>NeurIPS workshop on Interpretable Inductive Biases and Physically Structured Learning</em>, 2020.
</div>

<div class="rounded-box">
    <strong><a href="https://grlearning.github.io/papers/85.pdf" target="_blank">Group Anomaly Detection via Graph Autoencoders</a></strong><br>
    Pierluca D’Oro, Ennio Nasca, Jonathan Masci, Matteo Matteucci <br>
    <em>NeurIPS Graph Representation Learning Workshop</em>, 2019.
</div>

<div class="rounded-box">
    <strong><a href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11130/Palazzo_Generating_Synthetic_Video_Sequences_by_Explicitly_Modeling_Object_Motion_ECCVW_2018_paper.pdf" target="_blank">Generating Synthetic Video Sequences by Explicitly Modeling Object Motion</a></strong><br>
    Simone Palazzo, Concetto Spampinato, Pierluca D’Oro, Daniela Giordano, Mubarak Shah <br>
    <em>ECCV Workshop on Generating Realistic Visual Data of Human Behavior</em>, 2018.
</div>
